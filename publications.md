---
layout: page
permalink: /publications/index.html
title: Publications
---

## Publications

### A. Condensation phenomenon of deep learning
*Condensation phenomenon: Neurons in the same layer tends to align with one another during the training*

<img src="https://yaoyuzhang1.github.io/images/condensation.png" class="floatpic" style="width: 100%;">

#### A1. Regime of condensation—phase diagram series
1. Tao Luo, Zhi-Qin John Xu, Zheng Ma, Yaoyu Zhang, ["Phase Diagram for Two-layer ReLU Neural Networks at Infinite-Width Limit,"](https://yaoyuzhang1.github.io/file/A1/Phase%20diagram%20for%20two-layer%20relu%20neural%20networks%20at%20infinite-width%20limit.pdf) Journal of Machine Learning Research (JMLR) 22(71):1−47, (2021).
2. Hanxu Zhou, Qixuan Zhou, Zhenyuan Jin, Tao Luo, Yaoyu Zhang, Zhi-Qin John Xu, ["Empirical Phase Diagram for Three-layer Neural Networks with Infinite Width,"](https://yaoyuzhang1.github.io/file/A1/Empirical%20Phase%20Diagram%20for%20Three-layer%20Neural%20Networks%20with%20Infinite%20Width.pdf)  NeurIPS 2022.

#### A2. Loss landscape structure—embedding principle series
1. Yaoyu Zhang, Zhongwang Zhang, Tao Luo, Zhi-Qin John Xu, ["Embedding Principle of Loss Landscape of Deep Neural Networks,"](https://yaoyuzhang1.github.io/file/A2/Embedding%20Principle%20of%20Loss%20Landscape%20of%20Deep%20Neural%20Networks.pdf) NeurIPS 2021 spotlight.
2. Yaoyu Zhang, Yuqing Li, Zhongwang Zhang, Tao Luo, Zhi-Qin John Xu, ["Embedding Principle: a hierarchical structure of loss landscape of deep neural networks,"](https://yaoyuzhang1.github.io/file/A2/Embedding%20Principle%20a%20hierarchical%20structure%20of%20loss%20landscape%20of%20deep%20neural%20networks.pdf) Journal of Machine Learning, 1(1), pp. 60-113, 2022.
3. Hanxu Zhou, Qixuan Zhou, Tao Luo, Yaoyu Zhang, Zhi-Qin John Xu, ["Towards Understanding the Condensation of Neural Networks at Initial Training,"](https://yaoyuzhang1.github.io/file/A2/Towards%20Understanding%20the%20Condensation%20of%20Neural%20Networks%20at%20Initial%20training.pdf) NeurIPS 2022.
4. Zhiwei Bai, Tao Luo, Zhi-Qin John Xu, Yaoyu Zhang, ["Embedding Principle in Depth for the Loss Landscape Analysis of Deep Neural Networks,"](https://yaoyuzhang1.github.io/file/A2/Embedding%20Principle%20in%20Depth%20for%20the%20Loss%20Landscape%20Analysis%20of%20Deep%20neural%20networks.pdf) CSIAM Trans. Appl. Math., 5 (2024), pp. 350-389.
5. Leyang Zhang, Yaoyu Zhang, Tao Luo, ["Geometry of Critical Sets and Existence of Saddle Branches for Two-layer Neural Networks,"](https://yaoyuzhang1.github.io/file/A2/Geometry%20of%20Critical%20Sets%20and%20Existence%20of%20Saddle%20branches%20for%20two%20-layer%20Neural%20networks.pdf) arXiv:2405.17501 (2024).

#### A3. Generalization advantage—optimistic estimate series
1. Yaoyu Zhang, Zhongwang Zhang, Leyang Zhang, Zhiwei Bai, Tao Luo, Zhi-Qin John Xu, ["Linear Stability Hypothesis and Rank Stratification for Nonlinear Models,"](https://yaoyuzhang1.github.io/file/A3/Linear%20Stability%20Hypothesis%20and%20Rank%20Stratification%20for%20Nonlinear%20Models.pdf) arXiv:2211.11623 (2022).
2. Yaoyu Zhang, Zhongwang Zhang, Leyang Zhang, Zhiwei Bai, Tao Luo, Zhi-Qin John Xu, ["Optimistic Estimate Uncovers the Potential of Nonlinear Models,"](https://yaoyuzhang1.github.io/file/A3/Optimistic%20Estimate%20Uncovers%20the%20Potential%20of%20Nonlinear%20Models.pdf) arXiv:2307.08921 (2023).
3. Yaoyu Zhang, Leyang Zhang, Zhongwang Zhang, Zhiwei Bai, ["Local Linear Recovery Guarantee of Deep Neural Networks at Overparameterization,"](https://yaoyuzhang1.github.io/file/A3/Local%20Linear%20Recovery%20Guarantee%20of%20Deep%20Neural%20Networks%20at%20Overparameterization.pdf) arXiv:2406.18035 (2024).
4. Tao Luo, Leyang Zhang, Yaoyu Zhang, ["Structure and Gradient Dynamics Near Global Minima of Two-layer Neural Networks,"](https://yaoyuzhang1.github.io/file/A3/Structure%20and%20Gradient%20Dynamics%20Near%20Global%20Minima%20of%20Two-layer%20Neural%20Networks.pdf) arXiv:2309.00508 (2023).

#### A4. Global dynamics and implicit bias
1. Leyang Zhang, Zhi-Qin John Xu, Tao Luo, Yaoyu Zhang, ["Limitation of Characterizing Implicit Regularization by Data-independent Functions,"](https://yaoyuzhang1.github.io/file/A4/Limitation%20of%20Characterizing%20Implicit%20Regularization%20by%20Data-independent%20Functions.pdf) Transactions on Machine Learning Research (2023).
2. Zhiwei Bai, Jiajie Zhao, Yaoyu Zhang, ["Connectivity Shapes Implicit Regularization in Matrix Factorization Models for Matrix Completion"](https://yaoyuzhang1.github.io/file/A4/Connectivity%20Shapes%20Implicit%20Regularization%20in%20Matrix%20Factorization%20Models%20for%20Matrix%20Completion.pdf), NeurIPS 2024.
3. Jiajie Zhao, Zhiwei Bai, Yaoyu Zhang, ["Disentangle Sample Size and Initialization Effect on Perfect Generalization for Single-Neuron Target,"](https://yaoyuzhang1.github.io/file/A4/Disentangle%20Sample%20Size%20and%20Initialization%20Effect%20on%20Perfect%20Generalization%20for%20Single-Neuron%20Target.pdf) arXiv:2405.13787 (2024).

#### A5. Condensation in language models
1. Zhongwang Zhang, Pengxiao Lin, Zhiwei Wang, Yaoyu Zhang, Zhi-Qin John Xu, ["Initialization is Critical to Whether Transformers Fit Composite Functions by Inference or Memorizing,"](https://yaoyuzhang1.github.io/file/A5/Initialization%20is%20Critical%20to%20Whether%20Transformers%20Fit%20Composite%20Functions%20by%20Inference%20or%20Memorizing%2C.pdf) NeurIPS 2024.
2. Zhiwei Wang, Yunji Wang, Zhongwang Zhang, Zhangchen Zhou, Hui Jin, Tianyang Hu, Jiacheng Sun, Zhenguo Li, Yaoyu Zhang, Zhi-Qin John Xu, ["The Buffer Mechanism for Multi-Step Information Reasoning in Language Models"](https://yaoyuzhang1.github.io/file/A5/The%20Buffer%20Mechanism%20for%20Multi-Step%20Information%20Reasoning%20in%20Language%20Models.pdf), arXiv:2405.15302 (2024).
3. Zhongwang Zhang, Pengxiao Lin, Zhiwei Wang, Yaoyu Zhang, Zhi-Qin John Xu, ["Complexity Control Facilitates Reasoning-Based Compositional Generalization in Transformers"](https://yaoyuzhang1.github.io/file/A5/Complexity%20Control%20Facilitates%20Reasoning-Based%20Compositional%20Generalization%20in%20Transformers.pdf), arXiv:2501.08537 (2025).

### B. Frequency Principle of deep learning
*Frequency Principle: neural networks tend to learn from low to high frequencies during the training.*

1. **First Paper**: Zhiqin Xu, Yaoyu Zhang, Yanyang Xiao, ["Training Behavior of Deep Neural Network in Frequency Domain,"](https://yaoyuzhang1.github.io/file/B/Training%20Behavior%20of%20Deep%20Neural%20Network%20in%20Frequency%20Domain.pdf) ICONIP, pp. 264-274, 2019. (arXiv:1807.01251, Jul 2018)
2. **2021 World Artificial Intelligence Conference Youth Outstanding Paper Nomination Award**: Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, Zheng Ma, ["Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks,"](https://yaoyuzhang1.github.io/file/B/Frequency%20Principle%20Fourier%20Analysis%20Sheds%20Light%20on%20Deep%20Neural%20Networks.pdf) CiCP 28(5). 1746-1767, 2020.
3. **Initialization effect**: Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, Zheng Ma, ["A Type of Generalization Error Induced by Initialization in Deep Neural Networks,"](https://yaoyuzhang1.github.io/file/B/A%20Type%20of%20Generalization%20Error%20Induced%20by%20Initialization%20in%20Deep%20Neural%20Networks.pdf) MSML 2020.
4. **Linear Frequency Principle**: Yaoyu Zhang, Tao Luo, Zheng Ma, Zhi-Qin John Xu, ["Linear Frequency Principle Model to Understand the Absence of Overfitting in Neural Networks,"](https://yaoyuzhang1.github.io/file/B/Linear%20Frequency%20Principle%20Model%20to%20Understand%20the%20Absence%20of%20Overfitting%20in%20Neural%20Networks.pdf) Chinese Physics Letters (CPL) 38(3), 038701, 2021.
5. Tao Luo, Zheng Ma, Zhi-Qin John Xu, Yaoyu Zhang, ["Theory of the Frequency Principle for General Deep Neural Networks,"](https://yaoyuzhang1.github.io/file/B/Theory%20of%20the%20Frequency%20Principle%20for%20General%20Deep%20Neural%20Networks.pdf) CSIAM Trans. Appl. Math. 2 (2021), pp. 484-507.
6. **Linear Frequency Principle**: Tao Luo, Zheng Ma, Zhi-Qin John Xu, Yaoyu Zhang, On the exact computation of linear frequency principle dynamics and its generalization, SIAM Journal on Mathematics of Data Science 4 (4), 1272-1292, 2022.
7. **Minimal decay in frequency domain**: Tao Luo, Zheng Ma, Zhiwei Wang, Zhi-Qin John Xu, Yaoyu Zhang, ["An Upper Limit of Decaying Rate with Respect to Frequency in Deep Neural Network,"](https://yaoyuzhang1.github.io/file/B/An%20Upper%20Limit%20of%20Decaying%20Rate%20with%20Respect%20to%20Frequency%20in%20Deep%20Neural%20Network.pdf) MSML 2022.
8. **Overview**: Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, ["Overview Frequency Principle/Spectral Bias in Deep Learning,"](https://yaoyuzhang1.github.io/file/B/Overview%20Frequency%20Principle%20Spectral%20Bias%20in%20Deep%20Learning.pdf) Communications on Applied Mathematics and Computation (2024): 1-38.
9. Zhangchen Zhou, Yaoyu Zhang, Zhi-Qin John Xu, ["A rationale from frequency perspective for grokking in training neural network,"](https://yaoyuzhang1.github.io/file/B/A%20rationale%20from%20frequency%20perspective%20for%20grokking%20in%20training%20neural%20network.pdf) arXiv:2405.17479 (2024).

### C. Deep Learning for Science
- Zhiwei Wang, Yaoyu Zhang, Pengxiao Lin, Enhan Zhao, E. Weinan, Tianhan Zhang, Zhi-Qin John Xu, ["Deep Mechanism Reduction (DeePMR) Method for Fuel Chemical Kinetics,"](https://yaoyuzhang1.github.io/file/C/Deep%20Mechanism%20Reduction%20%28DeePMR%29%20Method%20for%20Fuel%20Chemical%20Kinetics.pdf) Method for Fuel Chemical Kinetics.pdf) Combustion and Flame 261 (2024): 113286.
- Tianhan Zhang, Yuxiao Yi, Yifan Xu, Zhi X. Chen, Yaoyu Zhang, Weinan E, Zhi-Qin John Xu, ["A Multi-scale Sampling Method for Accurate and Robust Deep Neural Network to Predict Combustion Chemical Kinetics,"](https://yaoyuzhang1.github.io/file/C/A%20Multi-scale%20Sampling%20Method%20for%20Accurate%20and%20Robust%20Deep%20Neural%20Network%20to%20Predict%20Combustion%20Chemical%20Kinetics.pdf) Combustion and Flame, 245, 112319, 2022.
- Lulu Zhang, Zhi-Qin John Xu, Yaoyu Zhang, ["Data-informed Deep Optimization,"](https://yaoyuzhang1.github.io/file/C/Data-informed%20Deep%20Optimization.pdf) PLoS ONE 17 (6), e0270191, 2022.
-  Jihong Wang, Zhi-Qin John Xu, Jiwei Zhang, Yaoyu Zhang, ["Implicit Bias with Ritz-Galerkin Method in Understanding Deep Learning for Solving PDEs,"](https://yaoyuzhang1.github.io/file/C/Implicit%20Bias%20with%20Ritz-Galerkin%20Method%20in%20Understanding%20Deep%20Learning%20for%20Solving%20PDEs.pdf) CSIAM Trans. Appl. Math. 3(2), pp. 299-317, 2022.
- Zhiwei Wang, Yaoyu Zhang, Yiguang Ju, Weinan E, Zhi-Qin John Xu, Tianhan Zhang, ["A Deep Learning-based Model Reduction (DeePMR) Method for Simplifying Chemical Kinetics,"](https://yaoyuzhang1.github.io/file/C/Adeep%20learning-based%20model%20reduction%20%28DeePMR%29%20method%20for%20simplifying%20chemical%20kinetics.pdf) method for simplifying chemical kinetics.pdf) arXiv:2201.02025 (2022).
- Lulu Zhang, Tao Luo, Yaoyu Zhang, Weinan E, Zhi-Qin John Xu, Zheng Ma, ["MOD-Net: A Machine Learning Approach via Model-Operator-Data Network for Solving PDEs,"](https://yaoyuzhang1.github.io/file/C/MOD-Net%20%20A%20Machine%20Learning%20Approach%20via%20Model-Operator-Data%20Network%20for%20Solving%20PDEs.pdf) Communications in Computational Physics 32(2) 299-335 2022.
- Tianhan Zhang, Yaoyu Zhang, Weinan E, Yiguang Ju, ["DLODE: A Deep Learning-based ODE Solver for Chemistry Kinetics,"](https://yaoyuzhang1.github.io/file/C/Adeep%20learning-based%20model%20reduction%20%28DeePMR%29%20method%20for%20simplifying%20chemical%20kinetics.pdf) method for simplifying chemical kinetics.pdf) AIAA Scitech 2021 Forum, 1139.

### D. Computational Neuroscience
- Jing Yan, Yunxuan Feng, Wei Dai, Yaoyu Zhang, ["State-dependent Filtering of the Ring Model,"](https://yaoyuzhang1.github.io/file/D/State-dependent%20Filtering%20of%20the%20Ring%20Model.pdf) arXiv:2408.01817 (2024). 
- Yaoyu Zhang, Lai-Sang Young, ["DNN-Assisted Statistical Analysis of a Model of Local Cortical Circuits,"](https://yaoyuzhang1.github.io/file/D/DNN-Assisted%20Statistical%20Analysis%20of%20a%20Model%20of%20Local%20Cortical%20Circuits.pdf) Scientific Reports 10, 20139, 2020.
- Yaoyu Zhang, Yanyang Xiao, Douglas Zhou, David Cai, ["Spike-Triggered Regression for Synaptic Connectivity Reconstruction in Neuronal Networks,"](https://yaoyuzhang1.github.io/file/D/Spike-Triggered%20Regression%20for%20Synaptic%20Connectivity%20Reconstruction%20in%20Neuronal%20Networks.pdf) Frontiers in Computational Neuroscience 11, 101, 2017.
- Yaoyu Zhang, Yanyang Xiao, Douglas Zhou, David Cai, ["Granger Causality Analysis with Nonuniform Sampling and Its Application to Pulse-coupled Nonlinear Dynamics,"](https://yaoyuzhang1.github.io/file/D/Granger%20Causality%20Analysis%20with%20Nonuniform%20Sampling%20and%20Its%20Application%20to%20Pulse-coupled%20Nonlinear%20Dynamics.pdf) Physical Review E 93, 042217, 2016.
- Douglas Zhou, Yaoyu Zhang, Yanyang Xiao, David Cai, ["Analysis of Sampling Artifacts on the Granger Causality Analysis for Topology Extraction of Neuronal Dynamics,"](https://yaoyuzhang1.github.io/file/D/Analysis%20of%20Sampling%20Artifacts%20on%20the%20Granger%20Causality%20Analysis%20for%20Topology%20Extraction%20of%20Neuronal%20Dynamics.pdf) Frontiers in Computational Neuroscience 8, 75, 2014.
- Douglas Zhou, Yaoyu Zhang, Yanyang Xiao, David Cai, ["Reliability of the Granger Causality Inference,"](https://yaoyuzhang1.github.io/file/D/Reliability%20of%20the%20Granger%20Causality%20Inference.pdf) New Journal of Physics 16 (4), 043016, 2014.
- Douglas Zhou, Yanyang Xiao, Yaoyu Zhang, Zhiqin Xu, David Cai, ["Granger Causality Network Reconstruction of Conductance-Based Integrate-and-Fire Neuronal Systems,"](https://yaoyuzhang1.github.io/file/D/Granger%20Causality%20Network%20Reconstruction%20of%20Conductance-Based%20Integrate-and-Fire%20Neuronal%20Systems.pdf) PloS One 9 (2), e87636, 2014.
- Douglas Zhou, Yanyang Xiao, Yaoyu Zhang, Zhiqin Xu, David Cai, ["Causal and Structural Connectivity of Pulse-coupled Nonlinear Networks,"](https://yaoyuzhang1.github.io/file/D/Causal%20and%20Structural%20Connectivity%20of%20Pulse-coupled%20Nonlinear%20Networks.pdf) Physical Review Letters 111 (5), 054102, 2013.

