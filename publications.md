---
layout: page
permalink: /publications/index.html
title: Publications
---

## Publications
#### See Google Scholar for more recent works: [[web]](https://scholar.google.com/citations?hl=en&user=ZSZ0TkIAAAAJ&view_op=list_works&sortby=pubdate)
### Highlights in deep learning theory
### **Optimistic Estimate (generalization)**
#### **Optimistic Estimate**: estimate of the smallest possible sample size for recovering a target for nonlinear regression.
#### **[Short paper]** **Yaoyu Zhang***, Zhongwang Zhang, Leyang Zhang, Zhiwei Bai, Tao Luo, Zhi-Qin John Xu*, Optimistic Estimate Uncovers the Potential of Nonlinear Models. arXiv:2307.08921, (2023). [[web]](https://arxiv.org/abs/2307.08921) [[pdf]](https://arxiv.org/pdf/2307.08921)
#### *A conceptual leap from long paper. Establish optimistic estimate and estimate the optimistic sample size for deep models, matrix factorization model and DNNs.*
#### **[initial paper]** **Yaoyu Zhang***, Zhongwang Zhang, Leyang Zhang, Zhiwei Bai, Tao Luo, Zhi-Qin John Xu*, Linear Stability Hypothesis and Rank Stratification for Nonlinear Models. arXiv:2211.11623, (2022). [[web]](https://arxiv.org/abs/2211.11623) [[pdf]](https://arxiv.org/pdf/2211.11623)
#### *An early manuscript containing many detailed technical results, but lack the key concept of optimistic estimate.*
#### **[Long paper]** **Yaoyu Zhang***, Leyang Zhang, Zhongwang Zhang and Zhiwei Bai, Local Linear Recovery Guarantee of Deep Neural Networks at Overparameterization. arXiv:2406.18035, (2024). [[web]](https://arxiv.org/abs/2406.18035) [[pdf]](https://arxiv.org/pdf/2406.18035)
#### *Long paper proposing the notion of local linear recovery and providing the theoretical details of optimistic sample size estimate.*
#### **[Local recovery]** Leyang Zhang, **Yaoyu Zhang**, Tao Luo, Structure and Gradient Dynamics Near Global Minima of Two-layer Neural Networks. arXiv:2309.00508, (2023). [[web]](https://arxiv.org/abs/2309.00508) [[pdf]](https://arxiv.org/pdf/2309.00508)
#### *Long paper analyzing the branching geometry of global minima and providing the required sample sizes for local recovery.*

### **Embedding Principle (loss landscape)**
#### **Embedding Principle (in width)**: loss landscape of any DNN contains all critical points of all narrower DNNs.
#### **[Short paper]** **Yaoyu Zhang***, Zhongwang Zhang, Tao Luo, Zhi-Qin John Xu*, Embedding Principle of Loss Landscape of Deep Neural Networks. NeurIPS 2021 spotlight. [[web]](https://arxiv.org/abs/2105.14573) [[pdf]](https://arxiv.org/pdf/2105.14573)
#### *Prove the Embedding Principle by the multi-step compositional embedding and unravel its practical implications to optimization, training&generalization and pruning.*
#### **[Long paper]** **Yaoyu Zhang***, Yuqing Li, Zhongwang Zhang, Tao Luo, Zhi-Qin John Xu*, Embedding Principle: a hierarchical structure of loss landscape of deep neural networks. Journal of Machine Learning, 1(1), pp. 60-113, 2022. [[web]](https://www.global-sci.org/intro/article_detail/jml/20372.html) [[pdf]](https://doc.global-sci.org/uploads/Issue/JML/v1n1/11_60.pdf?code=jgIBQOHO6qHAP%2F%2BoXwATUw%3D%3D)
#### *Extend the results in short paper, formally define the critical embedding and discover a wider class of general compatible critical embeddings.*
#### **[Embedding Principle in depth]** Zhiwei Bai, Tao Luo, Zhi-Qin John Xu*, **Yaoyu Zhang***, Embedding Principle in Depth for the Loss Landscape Analysis of Deep Neural Networks, 	CSIAM Trans. Appl. Math., 5 (2024), pp. 350-389. arXiv:2205.13283, (2022). [[web]](https://arxiv.org/abs/2205.13283) [[pdf]](https://arxiv.org/pdf/2205.13283) 
#### *Establish the Embedding Principle in depth.*

### **Phase diagram (dynamics)**
#### **Phase diagram**: a diagram showing the transition between linear (NTK/kernel/lazy) regime, critical (mean-field) regime or condensed regime depending on initialization hyperparameters for NNs at the infinite-width limit.
#### **[Two-layer]** Tao Luo#, Zhi-Qin John Xu#, Zheng Ma, **Yaoyu Zhang***,  Phase diagram for two-layer ReLU neural networks at infinite-width limit, Journal of Machine Learning Research (JMLR) 22(71):1−47, (2021) [[web]](https://jmlr.csail.mit.edu/papers/v22/20-1123.html) [[pdf]](https://jmlr.csail.mit.edu/papers/volume22/20-1123/20-1123.pdf)
#### *A map for realizing different training and implicit regularization effect.*
#### **[Three-layer]** Hanxu Zhou, Zhou Qixuan, Zhenyuan Jin, Tao Luo, **Yaoyu Zhang**, Zhi-Qin Xu*, Empirical phase diagram for three-layer neural networks with infinite width, NeurIPS 2022 [[pdf]](https://proceedings.neurips.cc/paper_files/paper/2022/file/a71c1931d3fb8ba564f7458d0657d0b1-Paper-Conference.pdf).

### **Frequency Principle (dynamics&implicit bias)**
#### **Frequency Principle**: DNNs often learn target function from low to high frequencies. 
#### **[First paper]** Zhiqin Xu, **Yaoyu Zhang**, Yanyang Xiao, Training Behavior of Deep Neural Network in Frequency Domain, International Conference on Neural Information Processing (ICONIP), pp. 264-274, 2019. (arXiv:1807.01251, Jul 2018) [[web]](https://arxiv.org/abs/1807.01251) [[pdf]](https://arxiv.org/pdf/1807.01251)
#### *Empirically discovering the Frequency Principle in simple datasets (specifically 1-d sythetic data).*
#### **[2021 World Artificial Intelligence Conference Youth Outstanding Paper Nomination Award]** Zhi-Qin John Xu*, **Yaoyu Zhang**, Tao Luo, Yanyang Xiao, Zheng Ma, Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks, Communications in Computational Physics (CiCP) 28(5). 1746-1767, 2020. [[web]](https://arxiv.org/abs/1901.06523) [[pdf]](https://arxiv.org/pdf/1901.06523)
#### *Extensively demonstrate the Frequency Principle in high-dimensional real datasets with an intuitive theoretical explanation.*
#### **[Linear Frequency Principle]** **Yaoyu Zhang**, Tao Luo, Zheng Ma, Zhi-Qin John Xu*, Linear Frequency Principle Model to Understand the Absence of Overfitting in Neural Networks, Chinese Physics Letters (CPL) 38(3), 038701, 2021. [[web]](https://iopscience.iop.org/article/10.1088/0256-307X/38/3/038701) [[pdf]](https://iopscience.iop.org/article/10.1088/0256-307X/38/3/038701/pdf)
#### *Propose a linear frequency principle (LFP) model to quantitatively understand the training and generalization consequence of the Frequency Principle.*
#### **[Overview]** Zhi-Qin John Xu*, **Yaoyu Zhang**, Tao Luo, Overview frequency principle/spectral bias in deep learning. arxiv 2201.07395 (2022) [[web]](https://arxiv.org/abs/2201.07395) [[pdf]](https://arxiv.org/pdf/2201.07395)

### Deep learning theory
#### •	**[Implicit bias characterization]** Leyang Zhang, Zhi-Qin John Xu, Tao Luo*, **Yaoyu Zhang***, Limitation of Characterizing Implicit Regularization by Data-independent Functions, Transactions on Machine Learning Research 2023. [[web]](https://openreview.net/forum?id=140kSqm0uy) [[pdf]](https://openreview.net/pdf?id=140kSqm0uy)
#### •	**[Initial condensation]** Zhi-Qin John Xu*#, Hanxu Zhou#, Tao Luo, **Yaoyu Zhang***, Towards Understanding the Condensation of Two-layer Neural Networks at Initial Training. NeurIPS 2022 [[pdf]](https://proceedings.neurips.cc/paper_files/paper/2022/file/0f4d1fc085b7504c140e66bb26ed8842-Paper-Conference.pdf) ;arXiv:2105.11686, (2021) [[web]](https://arxiv.org/abs/2105.11686) [[pdf]](https://arxiv.org/pdf/2105.11686)
#### •	**[DNN vs. Ritz-Galerkin]** (Alphabetic order) Jihong Wang, Zhi-Qin John Xu*, Jiwei Zhang*, **Yaoyu Zhang**, Implicit bias with Ritz-Galerkin method in understanding deep learning for solving PDEs, CSIAM Trans. Appl. Math. 3(2) 299-317, 2022 [[pdf]](https://doc.global-sci.org/uploads/Issue/CSIAM-AM/shortpdf/v3n2/32_299.pdf); arXiv:2002.07989, 2020. [[web]](https://arxiv.org/abs/2002.07989) [[pdf]](https://arxiv.org/pdf/2002.07989)
#### •	**[F-Principle theory for general DNN]** (Alphabetic order) Tao Luo, Zheng Ma, Zhi-Qin John Xu, **Yaoyu Zhang**, Theory of the Frequency Principle for General Deep Neural Networks, CSIAM Trans. Appl. Math. 2(3) 484-507, 2021 [[pdf]](https://doc.global-sci.org/uploads/Issue/CSIAM-AM/shortpdf/v2n3/23_484.pdf); [[web]](https://arxiv.org/abs/1906.09235) [[pdf]](https://arxiv.org/pdf/1906.09235)
#### •	**[Initialization effect]** **Yaoyu Zhang**, Zhi-Qin John Xu*, Tao Luo, Zheng Ma, A type of generalization error induced by initialization in deep neural networks, Mathematical and Scientific Machine Learning (MSML), 2020. [[web]](http://proceedings.mlr.press/v107/zhang20a.html) [[pdf]](http://proceedings.mlr.press/v107/zhang20a/zhang20a.pdf)
#### •	**[Derivation of LFP model]** (Alphabetic order) Tao Luo, Zheng Ma, Zhi-Qin John Xu, **Yaoyu Zhang**, On the exact computation of linear frequency principle dynamics and its generalization, SIAM Journal on Mathematics of Data Science 4 (4), 1272-1292, 2022 [[web]](https://epubs.siam.org/doi/10.1137/21M1444400)[[pdf]](https://ins.sjtu.edu.cn/people/xuzhiqin/pub/LFPexact2010.08153.pdf);arXiv:2010.08153, 2020. [[web]](https://arxiv.org/abs/2010.08153) [[pdf]](https://arxiv.org/pdf/2010.08153)
#### •	**[Limit convergence rate decay for Frequency Principle]** (Alphabetic order) Tao Luo, Zheng Ma, Zhiwei Wang, Zhi-Qin John Xu, and Yaoyu Zhang. Fourier-domain Variational Formulation and Its Well-posedness for Supervised Learning. arXiv:2012.03238, 2020. [[web]](https://arxiv.org/abs/2012.03238) [[pdf]](https://arxiv.org/pdf/2012.03238). See also: An Upper Limit of Decaying Rate with Respect to Frequency in Deep Neural Network. MSML 2022 [[pdf]](https://proceedings.mlr.press/v190/luo22a/luo22a.pdf); arXiv:2105.11675, 2020. [[web]](https://arxiv.org/abs/2105.11675) [[pdf]](https://arxiv.org/pdf/2105.11675)

### Deep learning in application
#### •	**[DiDo for industrial design]** Lulu Zhang, Zhi-Qin John Xu*, **Yaoyu Zhang***,Data-informed Deep Optimization, PLOS ONE 17 (6), e0270191 2022 [[web]](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0270191); arXiv:2107.08166, (2021). [[web]](https://arxiv.org/abs/2107.08166) [[pdf]](https://arxiv.org/pdf/2107.08166)
#### •	**[MOD-Net for PDEs]** Lulu Zhang, Tao Luo, **Yaoyu Zhang**, Zhi-Qin John Xu*, Zheng Ma*, MOD-Net: A Machine Learning Approach via Model-Operator-Data Network for Solving PDEs, Commun. Comput. Phys. 32(2) 299-335 2022 [[pdf]](https://doc.global-sci.org/uploads/Issue/CiCP/shortpdf/v32n2/322_299.pdf); arXiv:2107.03673, (2021). [[web]](https://arxiv.org/abs/2107.03673) [[pdf]](https://arxiv.org/pdf/2107.03673)
#### •	**[DeepCombustion0.0 for combustion simulation]** Tianhan Zhang, **Yaoyu Zhang***, Weinan E, Yiguang Ju, A deep learning-based ODE solver for chemical kinetics, arXiv:2012.12654, (2020). [[web]](https://arxiv.org/abs/2012.12654) [[pdf]](https://arxiv.org/pdf/2012.12654)（Zhang, Tianhan, Yaoyu Zhang, E. Weinan, and Yiguang Ju. "A deep learning-based ode solver for chemical kinetics." In AIAA Science and Technology Forum and Exposition, AIAA SciTech Forum 2021. American Institute of Aeronautics and Astronautics Inc, AIAA, 2021.）
#### • **[DeePMR for chemical kinetics reduction]** Zhiwei Wang, **Yaoyu Zhang**, Yiguang Ju, Weinan E, Zhi-Qin John Xu*, Tianhan Zhang*, A deep learning-based model reduction (DeePMR) method for simplifying chemical kinetics, arXiv:2201.02025, (2021). [[web]](https://arxiv.org/abs/2201.02025) [[pdf]](https://arxiv.org/pdf/2201.02025)
#### • Tianhan Zhang*, Yuxiao Yi, Yifan Xu, Zhi X Chen, Yaoyu Zhang, E Weinan, Zhi-Qin John Xu*, A multi-scale sampling method for accurate and robust deep neural network to predict combustion chemical kinetics, Combustion and Flame, 245, 112319, 2022 [[pdf]](https://ins.sjtu.edu.cn/people/xuzhiqin/pub/odechem2201.03549.pdf)

### Computational neuroscience
#### •	**Yaoyu Zhang**, Lai-Sang Young, DNN-Assisted Statistical Analysis of a Model of Local Cortical Circuits, Scientific Reports 10, 20139, 2020.
#### •	**Yaoyu Zhang**, Yanyang Xiao, Douglas Zhou, David Cai, Spike-Triggered Regression for Synaptic Connectivity Reconstruction in Neuronal Networks, Frontiers in Computational Neuroscience 11, 101, 2017.
#### •	**Yaoyu Zhang**, Yanyang Xiao, Douglas Zhou, David Cai, Granger Causality Analysis with Nonuniform Sampling and Its Application to Pulse-coupled Nonlinear Dynamics, Physical Review E 93, 042217, 2016.
#### •	Douglas Zhou, **Yaoyu Zhang**, Yanyang Xiao, David Cai, Analysis of Sampling Artifacts on the Granger Causality Analysis for Topology Extraction of Neuronal Dynamics, Frontiers in Computational Neuroscience 8, 75, 2014.
#### •	Douglas Zhou, **Yaoyu Zhang**, Yanyang Xiao, David Cai, Reliability of the Granger Causality Inference, New Journal of Physics 16 (4), 043016, 2014. 
#### •	Douglas Zhou, Yanyang Xiao, **Yaoyu Zhang**, Zhiqin Xu, David Cai, Granger Causality Network Reconstruction of Conductance-Based Integrate-and-Fire Neuronal Systems, PloS One 9 (2), e87636, 2014.
#### •	Douglas Zhou, Yanyang Xiao, **Yaoyu Zhang**, Zhiqin Xu, David Cai, Causal and Structural Connectivity of Pulse-coupled Nonlinear Networks, Physical Review Letters 111 (5), 054102, 2013.
